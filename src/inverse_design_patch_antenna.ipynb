{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Inverse Design of Patch Antennas - Google Colab Ready Version\n",
        "# ======================================================\n",
        "\n",
        "# 1. Environment Setup with proper installation\n",
        "# ---------------------\n",
        "!pip install -q tensorflow numpy pandas matplotlib scikit-learn keras-tuner gradio joblib\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import keras_tuner as kt\n",
        "import gradio as gr\n",
        "import json\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "# Set random seeds for full reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras Tuner version: {kt.__version__}\")\n",
        "\n",
        "# 2. Dataset Generation (Physics-Constrained)\n",
        "# --------------------------------------------\n",
        "def generate_antenna_dataset(num_samples=30000):\n",
        "    c = 3e8  # Speed of light (m/s)\n",
        "\n",
        "    # Design parameters\n",
        "    f = np.random.uniform(1e9, 12e9, size=num_samples)\n",
        "    er = np.random.uniform(2.2, 12.0, size=num_samples)\n",
        "    h = np.random.uniform(0.5e-3, 3e-3, size=num_samples)\n",
        "    loss_tangent = np.random.uniform(0.0001, 0.02, size=num_samples)\n",
        "\n",
        "    # Analytical formulas for patch antenna\n",
        "    W = np.clip(c / (2*f) * np.sqrt(2/(er+1)), 1e-3, 300e-3)\n",
        "    eps_eff = (er+1)/2 + (er-1)/(2*np.sqrt(1+12*h/W))\n",
        "    delta_L = 0.412*h*((eps_eff+0.3)*(W/h+0.264))/((eps_eff-0.258)*(W/h+0.8))\n",
        "    L = np.clip(c / (2*f*np.sqrt(eps_eff)) - 2*delta_L, 1e-3, 300e-3)\n",
        "\n",
        "    # Add realistic fabrication noise\n",
        "    W_noisy = W * np.random.normal(1.0, 0.015, size=num_samples)\n",
        "    L_noisy = L * np.random.normal(1.0, 0.015, size=num_samples)\n",
        "\n",
        "    # Build dataset with physical enforcement: W >= 1.05*L\n",
        "    data = pd.DataFrame({\n",
        "        'f_GHz': f/1e9,\n",
        "        'W_mm': np.maximum(W_noisy*1e3, L_noisy*1e3 * 1.05),\n",
        "        'L_mm': L_noisy*1e3,\n",
        "        'er': er,\n",
        "        'h_mm': h*1e3,\n",
        "        'loss_tangent': loss_tangent\n",
        "    })\n",
        "\n",
        "    return data\n",
        "\n",
        "print(\"üìä Generating dataset...\")\n",
        "data = generate_antenna_dataset(1000)  # Smaller dataset for quick testing\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(data.head())\n",
        "\n",
        "# 3. Data Preparation\n",
        "# --------------------\n",
        "X = data[['f_GHz', 'er']].values\n",
        "y = data[['L_mm', 'W_mm']].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_scaler = StandardScaler().fit(X_train)\n",
        "y_scaler = StandardScaler().fit(y_train)\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úÖ Data preparation complete!\")\n",
        "\n",
        "# 4. Model Definition (Physics-Aware Loss)\n",
        "# ----------------------------------------\n",
        "def custom_loss(y_true, y_pred):\n",
        "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "    constraint_penalty = tf.reduce_mean(tf.square(tf.maximum(1.05*y_pred[:,0] - y_pred[:,1], 0)))\n",
        "    return mse + 0.001 * constraint_penalty\n",
        "\n",
        "def constraint_satisfaction_metric(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.cast(y_pred[:,1] >= 1.05 * y_pred[:,0], tf.float32)) * 100\n",
        "\n",
        "# Simple model without hyperparameter tuning for quick testing\n",
        "def create_simple_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(256, activation='relu', input_shape=(2,)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(2)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=custom_loss,\n",
        "        metrics=['mae', constraint_satisfaction_metric]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"üß† Creating model...\")\n",
        "model = create_simple_model()\n",
        "model.summary()\n",
        "\n",
        "# 5. Training with Callbacks\n",
        "# ---------------------------\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=50,  # Reduced for quick testing\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. Evaluation\n",
        "# --------------\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Enforce constraint at prediction\n",
        "y_pred[:,1] = np.maximum(y_pred[:,1], y_pred[:,0]*1.05)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "compliance = constraint_satisfaction_metric(\n",
        "    tf.convert_to_tensor(y_test, dtype=tf.float32),\n",
        "    tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
        ").numpy()\n",
        "\n",
        "print(\"\\n=== üìä Evaluation Results ===\")\n",
        "print(f\"MAE: {mae:.3f} mm\")\n",
        "print(f\"R¬≤ Score: {r2:.4f}\")\n",
        "print(f\"Physics Compliance: {compliance:.2f}%\")\n",
        "\n",
        "# 7. Visualization\n",
        "# -----------------\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['constraint_satisfaction_metric'], label='Train Compliance')\n",
        "plt.plot(history.history['val_constraint_satisfaction_metric'], label='Val Compliance')\n",
        "plt.title('Physics Constraint Satisfaction (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 8. Save Model and Scalers\n",
        "# --------------------------\n",
        "!mkdir -p models\n",
        "\n",
        "model.save(\"models/patch_antenna_model.h5\")\n",
        "joblib.dump(X_scaler, \"models/x_scaler.pkl\")\n",
        "joblib.dump(y_scaler, \"models/y_scaler.pkl\")\n",
        "\n",
        "print(\"üíæ Model and scalers saved successfully!\")\n",
        "\n",
        "# 9. Gradio Interface\n",
        "# --------------------\n",
        "def plot_antenna(L, W):\n",
        "    fig, ax = plt.subplots(figsize=(6,4))\n",
        "    ax.add_patch(plt.Rectangle((0,0), L, W, fill=False, edgecolor='blue', linewidth=2))\n",
        "    ax.set_xlim(0, max(L,W)+10)\n",
        "    ax.set_ylim(0, max(L,W)+10)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_title(f\"Patch Antenna\\nL: {L:.1f} mm | W: {W:.1f} mm\")\n",
        "    ax.grid(True)\n",
        "    return fig\n",
        "\n",
        "def predict_patch(f_GHz, er):\n",
        "    X_input = X_scaler.transform([[f_GHz, er]])\n",
        "    pred = model.predict(X_input, verbose=0)\n",
        "    L, W = y_scaler.inverse_transform(pred)[0]\n",
        "    W = max(W, 1.05*L)  # Enforce physics constraint\n",
        "    text_output = f\"Predicted L = {L:.2f} mm, W = {W:.2f} mm\"\n",
        "    return text_output, plot_antenna(L, W)\n",
        "\n",
        "print(\"üåê Launching Gradio interface...\")\n",
        "iface = gr.Interface(\n",
        "    fn=predict_patch,\n",
        "    inputs=[\n",
        "        gr.Slider(1.0, 12.0, value=2.4, step=0.1, label=\"Frequency (GHz)\"),\n",
        "        gr.Slider(2.2, 12.0, value=4.4, step=0.1, label=\"Dielectric Constant (Œµ·µ£)\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Predicted Dimensions\"),\n",
        "        gr.Plot(label=\"Patch Antenna Geometry\")\n",
        "    ],\n",
        "    title=\"Patch Antenna Designer\",\n",
        "    description=\"Deep Learning-based Inverse Design | Physics-Constrained | Heavy Dataset\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "eXbVXB7muq_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}